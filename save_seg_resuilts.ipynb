{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ibug.face_detection import RetinaFacePredictor\n",
    "from ibug.face_parsing import FaceParser as RTNetPredictor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid stages [True, True, True]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.6 # default = 0.8\n",
    "weights = None # r\"C:\\mahmoud_dev\\machine learning\\segmentation\\face_parsing\\ibug\\face_parsing\\rtnet\\weights\\rtnet101-fcn-14.torch\" # default = None\n",
    "num_classes = 14 # default = 11\n",
    "max_num_faces = 50 # default = 50\n",
    "\n",
    "parser_encoder = 'rtnet101'\n",
    "parser_decoder = 'fcn'\n",
    "rotate_image = False\n",
    "save_json_file = True\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "face_detector = RetinaFacePredictor(threshold=threshold, device=device, model=(RetinaFacePredictor.get_model('mobilenet0.25')))\n",
    "face_parser = RTNetPredictor(device=device, ckpt=weights, encoder=parser_encoder, decoder=parser_decoder, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'background', 'id': 0, 'type': 'any', 'attributes': []}, {'name': 'skin', 'id': 1, 'type': 'any', 'attributes': []}, {'name': 'eyebrow', 'id': 2, 'type': 'any', 'attributes': []}, {'name': 'eye', 'id': 3, 'type': 'any', 'attributes': []}, {'name': 'hair', 'id': 4, 'type': 'any', 'attributes': []}, {'name': 'glasses', 'id': 5, 'type': 'any', 'attributes': []}, {'name': 'beard', 'id': 6, 'type': 'any', 'attributes': []}]\n"
     ]
    }
   ],
   "source": [
    "categories_list = ['background', 'skin', 'left_eyebrow', 'right_eyebrow', 'left_eye', 'right_eye',\n",
    "                 'nose', 'upper_lip', 'inner_mouth', 'lower_lip', 'hair']\n",
    "new_categories = ['background', 'skin', 'eyebrow', 'eye', 'hair', 'glasses', 'beard']\n",
    "\n",
    "if num_classes==14:\n",
    "    categories_list.extend(['left_ear', 'right_ear',  'glasses', 'beard'])\n",
    "\n",
    "categories = [{\"name\":category, \"id\":i, \"type\": \"any\", \"attributes\":[]} for i, category in enumerate(new_categories)]\n",
    "\n",
    "print(categories)\n",
    "# Initialize COCO format dictionary\n",
    "coco_dict = {\n",
    "    'info': {\"contributor\":\"mahmoud_tabikh\",\"date_created\":date.today().strftime('%d-%m-%Y'),\"description\":\"\", \"version\":\"\"},\n",
    "    'licenses': [],\n",
    "    'categories': categories,\n",
    "    'images': [],\n",
    "    'annotations': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_class_id(class_id):\n",
    "    new_categories = ['background', 'skin', 'eyebrow', 'eye', 'hair', 'glasses', 'beard']\n",
    "    og_category_dict = {category:id_ for id_, category in enumerate(categories_list)}\n",
    "    new_category_dict = {category:id_ for id_, category in enumerate(new_categories)}\n",
    "\n",
    "    to_skin = [og_category_dict[value] for value in [\"nose\", \"upper_lip\", \"inner_mouth\", \"lower_lip\", \"left_ear\", \"right_ear\"]]\n",
    "    to_single_brow = [og_category_dict[value] for value in [\"left_eyebrow\", \"right_eyebrow\"]]\n",
    "    to_single_eye = [og_category_dict[value] for value in [\"left_eye\", \"right_eye\"]]\n",
    "    identical_names = {og_category_dict[\"skin\"]:new_category_dict[\"skin\"],\n",
    "                       og_category_dict[\"hair\"]:new_category_dict[\"hair\"],\n",
    "                       og_category_dict[\"glasses\"]:new_category_dict[\"glasses\"]}\n",
    "\n",
    "    if class_id in to_skin:\n",
    "        return new_category_dict[\"skin\"]\n",
    "    elif class_id in to_single_brow:\n",
    "        return new_category_dict[\"eyebrow\"]\n",
    "    elif class_id in to_single_eye:\n",
    "        return new_category_dict[\"eye\"]\n",
    "    else:\n",
    "        return identical_names[class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different segmentation methods\n",
    "\n",
    "def get_segmenation_xy(mask, class_id):\n",
    "    # Create segmentation and bbox arrays\n",
    "    mask_bool = mask == class_id\n",
    "    area = np.sum(mask_bool)\n",
    "\n",
    "    ys, xs = np.where(mask_bool)\n",
    "    segmentation = np.asarray(list(zip(xs, ys))).flatten().tolist()\n",
    "    return area, segmentation\n",
    "\n",
    "def get_segmentation_countours(mask, class_id):\n",
    "    # Create segmentation and bbox arrays\n",
    "    mask_bool = mask == class_id\n",
    "    area = np.sum(mask_bool)\n",
    "\n",
    "    retrieval_method = cv2.RETR_EXTERNAL # options: cv2.RETR_EXTERNAL, cv2.RETR_TREE\n",
    "    contour_approximation = cv2.CHAIN_APPROX_SIMPLE # options: cv2.CHAIN_APPROX_SIMPLE, cv2.CHAIN_APPROX_NONE\n",
    "    contours, hierarchy = cv2.findContours(mask_bool.astype(np.uint8), retrieval_method, contour_approximation)\n",
    "    segmentations = []\n",
    "    for contour in contours:\n",
    "        if len(contour) < 3:\n",
    "            continue\n",
    "        segmentation = []\n",
    "        for point in contour:\n",
    "            segmentation.extend(point.flatten().tolist())\n",
    "        segmentations.append(segmentation)\n",
    "    return area, segmentations\n",
    "\n",
    "\n",
    "def get_segmentation_countours_test(mask, class_id):\n",
    "    # Create segmentation and bbox arrays\n",
    "    mask_bool = mask == class_id\n",
    "    area = np.sum(mask_bool)\n",
    "\n",
    "    retrieval_method = cv2.RETR_EXTERNAL # options: cv2.RETR_EXTERNAL, cv2.RETR_TREE\n",
    "    contour_approximation = cv2.CHAIN_APPROX_SIMPLE # options: cv2.CHAIN_APPROX_SIMPLE, cv2.CHAIN_APPROX_NONE\n",
    "    contours, hierarchy = cv2.findContours(mask_bool.astype(np.uint8), retrieval_method, contour_approximation)\n",
    "    segmentations = []\n",
    "    skin_classid = 1 # in trinamix new labels dict\n",
    "\n",
    "    if class_id != skin_classid:\n",
    "        for contour in contours:\n",
    "            if len(contour) < 3:\n",
    "                continue\n",
    "            segmentation = []\n",
    "            for point in contour:\n",
    "                segmentation.extend(point.flatten().tolist())\n",
    "            segmentations.append(segmentation)\n",
    "    else:\n",
    "        segmentations = [list(contour.flatten()) for contour in contours if len(contour) > 3]\n",
    "        if len(segmentations)>1:\n",
    "            segmentations = [[item for sublist in segmentations for item in sublist]]\n",
    "    return area, segmentations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_segmentation_coco(image_dir):\n",
    "    non_labelled = []\n",
    "    # Loop through images in directory\n",
    "    for image_id, filename in enumerate(tqdm((os.listdir(image_dir)))):\n",
    "        if filename.endswith(tuple([\".png\", \".jpg\"])):\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            image_id+=1\n",
    "            image = cv2.imread(image_path)\n",
    "            image_dict = { \n",
    "                'id': image_id,\n",
    "                'width': image.shape[1],\n",
    "                'height': image.shape[0],\n",
    "                'file_name': filename}\n",
    "            coco_dict['images'].append(image_dict)\n",
    "            try:\n",
    "\n",
    "                faces, masks = get_image_pred(image, face_detector, face_parser, filename)\n",
    "                mask, face = masks[0], faces[0] # assumes 1 face per image, loop for more faces.\n",
    "                annotations_list = get_annotations_list(mask, face, image_id, coco_dict[\"annotations\"])\n",
    "                for dict_ in annotations_list:\n",
    "                    coco_dict['annotations'].append(dict_)\n",
    "            except RuntimeError as e:\n",
    "                non_labelled.append(get_image_info(filename))\n",
    "\n",
    "    if save_json_file:\n",
    "        save_json(coco_dict)\n",
    "\n",
    "    return sorted(non_labelled)\n",
    "\n",
    "\n",
    "def get_image_pred(img, face_detector, face_parser, filename):\n",
    "    used_df = False\n",
    "    if rotate_image:\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    faces = face_detector(img)\n",
    "    masks = face_parser.predict_img(img, faces)\n",
    "    return faces, masks\n",
    "\n",
    "def get_annotations_list(mask, face, image_id, coco_annotation_dict):\n",
    "    # Create annotation dictionary for each unique mask value\n",
    "    annotation_list = [] # i am making a list of lists instead of list of dicts\n",
    "    og_category_dict = {category:id_ for id_, category in enumerate(categories_list)}\n",
    "    for class_id in np.unique(mask):\n",
    "        if class_id not in [og_category_dict[category] for category in [\"background\", \"nose\", \"left_ear\", \"right_ear\", \"upper_lip\", \"inner_mouth\", \"lower_lip\"]]:\n",
    "            annotation_id = len(coco_annotation_dict) + 1\n",
    "            annotation_dict = {\n",
    "                'id': annotation_id,\n",
    "                'image_id': image_id,\n",
    "                'category_id': convert_class_id(class_id),\n",
    "                'segmentation': [],\n",
    "                'bbox': face[:4].astype(int),\n",
    "                'iscrowd': 0}\n",
    "            area, segmentations = get_segmentation_countours(mask, class_id)\n",
    "            annotation_dict['segmentation'].extend(segmentations)\n",
    "            annotation_dict['area'] = area\n",
    "            annotation_list.append(annotation_dict)\n",
    "    # Add annotation to COCO dictionary\n",
    "    return annotation_list\n",
    "\n",
    "def save_json(coco_dict):\n",
    "    class NpEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            if isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "    Path(json_filepath).write_text(json.dumps(coco_dict, cls=NpEncoder, indent=3))\n",
    "    print(f\"json file written to {json_filepath}\")\n",
    "\n",
    "def get_image_info(image_filename):\n",
    "    image_nb = int(image_filename.split(\".\")[0])\n",
    "    return image_nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths and filenames\n",
    "image_dir = r'D:\\_Xchng\\Mahmoud\\segmenation\\DS01-segmentation_test\\data\\raw\\task_d_765_to_1019'\n",
    "json_filepath = os.path.join(image_dir, r'instances_default.json')\n",
    "txt_filepath = os.path.join(image_dir, r'non_labeled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [02:00<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json file written to D:\\_Xchng\\Mahmoud\\segmenation\\DS01-segmentation_test\\data\\raw\\task_d_765_to_1019\\instances_default.json\n"
     ]
    }
   ],
   "source": [
    "non_labelled = save_segmentation_coco(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019455252918287938"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ = len(os.listdir(image_dir))\n",
    "non_det = len(non_labelled)\n",
    "frac_non_det = non_det/all_\n",
    "frac_non_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_filepath, 'w') as f:\n",
    "    f.write(f\"non detected fraction: {frac_non_det}\\n\")\n",
    "    for line in sorted(non_labelled):\n",
    "        f.write(f\"{line}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_parse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
