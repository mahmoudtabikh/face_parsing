{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from PIL import Image\n",
    "from ibug.face_detection import RetinaFacePredictor\n",
    "from ibug.face_parsing import FaceParser as RTNetPredictor\n",
    "from ibug.face_parsing.utils import label_colormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8 # default = 0.8\n",
    "weights = None # r\"C:\\mahmoud_dev\\machine learning\\segmentation\\face_parsing\\ibug\\face_parsing\\rtnet\\weights\\rtnet101-fcn-14.torch\" # default = None\n",
    "num_classes = 14 # default = 11\n",
    "max_num_faces = 50 # default = 50\n",
    "\n",
    "parser_encoder = 'rtnet50'\n",
    "parser_decoder = 'fcn'\n",
    "\n",
    "root = r\"D:\\_Xchng\\Mahmoud\\segmenation\\dataset\\images\"\n",
    "save_root = r\"D:\\_Xchng\\Mahmoud\\segmenation\\dataset\\annotations\"\n",
    "\n",
    "rotate_image = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'name': 'background'},\n",
       " {'id': 1, 'name': 'skin'},\n",
       " {'id': 2, 'name': 'left_eyebrow'},\n",
       " {'id': 3, 'name': 'right_eyebrow'},\n",
       " {'id': 4, 'name': 'left_eye'},\n",
       " {'id': 5, 'name': 'right_eye'},\n",
       " {'id': 6, 'name': 'nose'},\n",
       " {'id': 7, 'name': 'upper_lip'},\n",
       " {'id': 8, 'name': 'inner_mouth'},\n",
       " {'id': 9, 'name': 'lower_lip'},\n",
       " {'id': 10, 'name': 'hair'},\n",
       " {'id': 11, 'name': 'left_ear'},\n",
       " {'id': 12, 'name': 'right_ear'},\n",
       " {'id': 13, 'name': 'glasses'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_coco_annotations():\n",
    "    # Label ids of the dataset\n",
    "    category_ids = {\n",
    "    \"background\": 0,\n",
    "    \"skin\": 1,\n",
    "    \"left_eyebrow\": 2,\n",
    "    \"right_eyebrow\": 3,\n",
    "    \"left_eye\": 4,\n",
    "    \"right_eye\": 5,\n",
    "    \"nose\": 6,\n",
    "    \"upper_lip\": 7,\n",
    "    \"inner_mouth\": 8,\n",
    "    \"lower_lip\": 9,\n",
    "    \"hair\": 10,\n",
    "    \"left_ear\": 11,\n",
    "    \"right_ear\": 12,\n",
    "    \"glasses\": 13\n",
    "    }\n",
    "\n",
    "    coco_annotations = {\n",
    "        \"info\": {},\n",
    "        \"licenses\": [],\n",
    "        \"categories\": [],\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "\n",
    "    for category_name, category_id in category_ids.items():\n",
    "        # Add category information to the COCO annotations dictionary\n",
    "        category_info = {\n",
    "            \"id\": category_id,\n",
    "            \"name\": category_name\n",
    "        }\n",
    "        coco_annotations[\"categories\"].append(category_info)\n",
    "    \n",
    "    return coco_annotations[\"categories\"]\n",
    "get_coco_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pred(img, face_detector, face_parser):\n",
    "    if rotate_image:\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    faces = face_detector(img, rgb=False)\n",
    "    masks = face_parser.predict_img(img, faces, rgb=False)\n",
    "    \n",
    "    return img, faces, masks\n",
    "\n",
    "\n",
    "def render_image(img_path, face_detector, face_parser):\n",
    "    colormap = label_colormap(num_classes)\n",
    "    img, faces, masks = get_image_pred(img_path, face_detector, face_parser)\n",
    "    alphas = np.linspace(0.75, 0.25, num=max_num_faces)\n",
    "\n",
    "    for i, (face, mask) in enumerate(zip(faces, masks)):\n",
    "        bbox = face[:4].astype(int)\n",
    "        cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color=(0, 0, 255), thickness=2)\n",
    "        alpha = alphas[i]\n",
    "        index = mask > 0\n",
    "        res = colormap[mask]\n",
    "        img[index] = (1 - alpha) * img[index].astype(float) + \\\n",
    "            alpha * res[index].astype(float)\n",
    "    img = np.clip(img.round(), 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def segment_images():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    face_detector = RetinaFacePredictor(threshold=threshold, device=device, model=(RetinaFacePredictor.get_model('mobilenet0.25')))\n",
    "    face_parser = RTNetPredictor(device=device, ckpt=weights, encoder=parser_encoder, decoder=parser_decoder, num_classes=num_classes)\n",
    "\n",
    "    for i, image_name in enumerate(os.listdir(root)):\n",
    "        image_path = os.path.join(root, image_name)\n",
    "\n",
    "        filename = image_name.split(\".\")\n",
    "        save_path = os.path.join(save_root, f\"{filename[0]}.json\")\n",
    "        \n",
    "        save_segmentation_result(i, filename[0], image_path, face_detector, face_parser, save_path)\n",
    "\n",
    "# segment_images()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid stages [True, True, True]\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "face_detector = RetinaFacePredictor(threshold=threshold, device=device, model=(RetinaFacePredictor.get_model('mobilenet0.25')))\n",
    "face_parser = RTNetPredictor(device=device, ckpt=weights, encoder=parser_encoder, decoder=parser_decoder, num_classes=num_classes)\n",
    "# specify the directory where the segmented images are stored\n",
    "segmented_images_dir = r'D:\\_Xchng\\Mahmoud\\segmenation\\dataset\\images'\n",
    "output_path = r'D:\\_Xchng\\Mahmoud\\segmenation\\dataset\\images\\annotations.json'\n",
    "# create a dictionary to map RGB values to label IDs for each label\n",
    "color_to_label = {\n",
    "    \"(0, 0, 0)\": 0, # background\n",
    "    \"(128, 0, 0)\": 1, # skin\n",
    "    \"(0, 128, 0)\": 2, # left_eyebrow\n",
    "    \"(128, 128, 0)\": 3, # right_eyebrow\n",
    "    \"(0, 0, 128)\": 4, # left_eye\n",
    "    \"(128, 0, 128)\": 5, # right_eye\n",
    "    \"(0, 128, 128)\": 6, # nose\n",
    "    \"(128, 128, 128)\": 7, # upper_lip\n",
    "    \"(64, 0, 0)\": 8, # inner_mouth\n",
    "    \"(192, 0, 0)\": 9, # lower_lip\n",
    "    \"(64, 128, 0)\": 10, # hair\n",
    "    \"(192, 128, 0)\": 11, # left_ear\n",
    "    \"(64, 0, 128)\": 12, # right_ear\n",
    "    \"(192, 0, 128)\": 13 # glasses\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(image)\n\u001b[0;32m     21\u001b[0m \u001b[39m# loop over each pixel in the image\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mprint\u001b[39m(color_to_label[masks[\u001b[39m0\u001b[39;49m]])\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(height):\n\u001b[0;32m     24\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(width):\n\u001b[0;32m     25\u001b[0m         \u001b[39m# get the color of the pixel\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "# create a list to store the annotations for each image\n",
    "annotations = []\n",
    "\n",
    "# loop over each image file in the directory\n",
    "for image_file in os.listdir(segmented_images_dir):\n",
    "    if image_file.endswith(\".png\"):\n",
    "        # open the image using PIL\n",
    "        image = cv2.imread(os.path.join(segmented_images_dir, image_file))\n",
    "        # get the image width and height\n",
    "        width, height = image.shape[0], image.shape[1]\n",
    "        # create a dictionary for the image annotation\n",
    "        image_annotation = {\n",
    "            'file_name': image_file,\n",
    "            'height': height,\n",
    "            'width': width,\n",
    "            'annotations': []\n",
    "        }\n",
    "        image, faces, masks = get_image_pred(image, face_detector, face_parser)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        # loop over each pixel in the image\n",
    "        print(color_to_label[masks[0]])\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                # get the color of the pixel\n",
    "                r, g, b = masks[0].getpixel((x, y))\n",
    "                # check if the pixel is part of a segmented object\n",
    "                if (r, g, b) in color_to_label:\n",
    "                    print(\"found it\")\n",
    "                    # get the label ID corresponding to the color of the pixel\n",
    "                    label_id = color_to_label[(r, g, b)]\n",
    "                    # create a dictionary for the segmentation annotation\n",
    "                    segmentation_annotation = {\n",
    "                        'segmentation': [[x, y, x+1, y, x+1, y+1, x, y+1]],\n",
    "                        'category_id': label_id,\n",
    "                        'iscrowd': 0\n",
    "                    }\n",
    "                    # append the segmentation annotation to the image annotation\n",
    "                    image_annotation['annotations'].append(segmentation_annotation)\n",
    "        # append the image annotation to the list of annotations\n",
    "        annotations.append(image_annotation)\n",
    "\n",
    "# create a dictionary for the COCO dataset\n",
    "dataset = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'categories': get_coco_annotations()}\n",
    "for i, image_annotation in enumerate(annotations):\n",
    "    # set the image ID\n",
    "    image_id = i + 1\n",
    "    # update the image annotation with the ID\n",
    "    image_annotation['id'] = image_id\n",
    "    # add the image annotation to the COCO dataset\n",
    "    dataset['images'].append(image_annotation)\n",
    "    # loop over the segmentation annotations in the image annotation and add them to the COCO dataset\n",
    "    for segmentation_annotation in image_annotation['annotations']:\n",
    "        # set the segmentation ID\n",
    "        segmentation_id = len(dataset['annotations']) + 1\n",
    "        # update the segmentation annotation with the image ID and ID\n",
    "        segmentation_annotation['image_id'] = image_id\n",
    "        segmentation_annotation['id'] = segmentation_id\n",
    "        # add the segmentation annotation to the COCO dataset\n",
    "        dataset['annotations'].append(segmentation_annotation)\n",
    "\n",
    "# save the COCO dataset dictionary to a JSON file\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(dataset, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_parse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
