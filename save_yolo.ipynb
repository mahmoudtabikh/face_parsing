{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from ibug.face_detection import RetinaFacePredictor\n",
    "from ibug.face_parsing import FaceParser as RTNetPredictor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8 # default = 0.8\n",
    "weights = None # r\"C:\\mahmoud_dev\\machine learning\\segmentation\\face_parsing\\ibug\\face_parsing\\rtnet\\weights\\rtnet101-fcn-14.torch\" # default = None\n",
    "num_classes = 14 # default = 11\n",
    "max_num_faces = 50 # default = 50\n",
    "\n",
    "parser_encoder = 'rtnet50'\n",
    "parser_decoder = 'fcn'\n",
    "rotate_image = False\n",
    "today = date.today()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "face_detector = RetinaFacePredictor(threshold=threshold, device=device, model=(RetinaFacePredictor.get_model('mobilenet0.25')))\n",
    "face_parser = RTNetPredictor(device=device, ckpt=weights, encoder=parser_encoder, decoder=parser_decoder, num_classes=num_classes)\n",
    "\n",
    "def get_image_pred(img, face_detector, face_parser):\n",
    "    if rotate_image:\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    faces = face_detector(img, rgb=False)\n",
    "    masks = face_parser.predict_img(img, faces, rgb=False)\n",
    "    \n",
    "    return faces, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = ['background', 'skin', 'left_eyebrow', 'nose', 'upper_lip', 'inner_mouth', 'lower_lip',\n",
    "                   'right_eyebrow', 'left_eye', 'hair', 'left_ear', 'right_ear', 'right_eye', 'glasses']\n",
    "\n",
    "class_ids = {categories_list[i]: i for i in range(len(categories_list))}\n",
    "print(class_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths and filenames\n",
    "image_dir = r'D:\\_Xchng\\Mahmoud\\segmenation\\dataset\\data\\raw_images'\n",
    "json_filepath = r'D:\\_Xchng\\Mahmoud\\segmenation\\dataset\\data\\instances_default.json'\n",
    "txt_filepath = r'D:\\_Xchng\\Mahmoud\\segmenation\\dataset\\data\\yolo_annotations\\obj_train_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yolo(image_dir):\n",
    "# Loop through images in directory\n",
    "    annotation = ''\n",
    "\n",
    "    for image_id, filename in enumerate(os.listdir(image_dir)):\n",
    "\n",
    "        if filename.endswith(tuple([\".png\", \".jpg\"])):\n",
    "\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            faces, masks = get_image_pred(image, face_detector, face_parser)\n",
    "            mask_arr, face = masks[0], faces[0] # assumes 1 face per image, loop for more faces.\n",
    "            for class_name in categories_list:\n",
    "                yolo_annotation = generate_segmentation_yolo(mask_arr, class_name, image, categories_list)\n",
    "                annotation += yolo_annotation\n",
    "\n",
    "        # Save annotation file to disk\n",
    "        values = filename.split(\".\")\n",
    "        path  = os.path.join(txt_filepath, f\"{values[0]}.txt\")\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(annotation)\n",
    "\n",
    "\n",
    "def generate_segmentation_yolo(mask, class_name, img, class_names):\n",
    "    annotation_seg = ''\n",
    "    width, height = img.shape[1], img.shape[0]\n",
    "    # Loop through all unique pixel values in the mask\n",
    "    for value in np.unique(mask):\n",
    "        # Skip background value\n",
    "        if value == 0:\n",
    "            continue\n",
    "    binary_mask = (mask > 0.5).astype(np.uint8)\n",
    "    # Get the contours of the mask\n",
    "    retrieval_method = cv2.RETR_TREE # options: cv2.RETR_EXTERNAL, cv2.RETR_TREE\n",
    "    contour_approximation = cv2.CHAIN_APPROX_NONE # options: cv2.CHAIN_APPROX_SIMPLE, cv2.CHAIN_APPROX_NONE\n",
    "    contours, hierarchy = cv2.findContours(binary_mask, retrieval_method, contour_approximation)\n",
    "    \n",
    "    # Define class names and map each class name to an index\n",
    "    class_dict = {class_name: index for index, class_name in enumerate(class_names)}\n",
    "    \n",
    "    # Write the bounding boxes to a txt file in YOLO format for each class\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        x_center = (x + w / 2) / width\n",
    "        y_center = (y + h / 2) / height\n",
    "        w_norm = w / width\n",
    "        h_norm = h / height\n",
    "        class_index = int(mask[y:y+h, x:x+w].mean() + 0.5)\n",
    "        class_name = class_names[class_index]\n",
    "        annotation_seg += f\"{class_dict[class_name]} {x_center} {y_center} {w_norm} {h_norm}\\n\"\n",
    "\n",
    "    \n",
    "    return annotation_seg"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
