{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\mahmoud_dev\\util\\venvs\\face_parse\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ibug.face_detection import RetinaFacePredictor\n",
    "from ibug.face_parsing import FaceParser as RTNetPredictor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid stages [True, True, True]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.6 # default = 0.8\n",
    "weights = None # r\"C:\\mahmoud_dev\\machine learning\\segmentation\\face_parsing\\ibug\\face_parsing\\rtnet\\weights\\rtnet101-fcn-14.torch\" # default = None\n",
    "num_classes = 14 # default = 11\n",
    "max_num_faces = 50 # default = 50\n",
    "\n",
    "parser_encoder = 'rtnet101'\n",
    "parser_decoder = 'fcn'\n",
    "rotate_image = False\n",
    "save_json_file = True\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "face_detector = RetinaFacePredictor(threshold=threshold, device=device, model=(RetinaFacePredictor.get_model('mobilenet0.25')))\n",
    "face_parser = RTNetPredictor(device=device, ckpt=weights, encoder=parser_encoder, decoder=parser_decoder, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'background', 'id': 0, 'type': 'any', 'attributes': []}, {'name': 'skin', 'id': 1, 'type': 'any', 'attributes': []}, {'name': 'eyebrow', 'id': 2, 'type': 'any', 'attributes': []}, {'name': 'eye', 'id': 3, 'type': 'any', 'attributes': []}, {'name': 'hair', 'id': 4, 'type': 'any', 'attributes': []}, {'name': 'glasses', 'id': 5, 'type': 'any', 'attributes': []}, {'name': 'beard', 'id': 6, 'type': 'any', 'attributes': []}]\n"
     ]
    }
   ],
   "source": [
    "categories_list = ['background', 'skin', 'left_eyebrow', 'right_eyebrow', 'left_eye', 'right_eye',\n",
    "                 'nose', 'upper_lip', 'inner_mouth', 'lower_lip', 'hair']\n",
    "new_categories = ['background', 'skin', 'eyebrow', 'eye', 'hair', 'glasses', 'beard']\n",
    "\n",
    "if num_classes==14:\n",
    "    categories_list.extend(['left_ear', 'right_ear',  'glasses', 'beard'])\n",
    "\n",
    "categories = [{\"name\":category, \"id\":i, \"type\": \"any\", \"attributes\":[]} for i, category in enumerate(new_categories)]\n",
    "\n",
    "print(categories)\n",
    "# Initialize COCO format dictionary\n",
    "coco_dict = {\n",
    "    'info': {\"contributor\":\"mahmoud_tabikh\",\"date_created\":date.today().strftime('%d-%m-%Y'),\"description\":\"\", \"version\":\"\"},\n",
    "    'licenses': [],\n",
    "    'categories': categories,\n",
    "    'images': [],\n",
    "    'annotations': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_class_id(class_id):\n",
    "    new_categories = ['background', 'skin', 'eyebrow', 'eye', 'hair', 'glasses', 'beard']\n",
    "    og_category_dict = {category:id_ for id_, category in enumerate(categories_list)}\n",
    "    new_category_dict = {category:id_ for id_, category in enumerate(new_categories)}\n",
    "\n",
    "    to_skin = [og_category_dict[value] for value in [\"nose\", \"upper_lip\", \"inner_mouth\", \"lower_lip\", \"left_ear\", \"right_ear\"]]\n",
    "    to_single_brow = [og_category_dict[value] for value in [\"left_eyebrow\", \"right_eyebrow\"]]\n",
    "    to_single_eye = [og_category_dict[value] for value in [\"left_eye\", \"right_eye\"]]\n",
    "    identical_names = {og_category_dict[\"skin\"]:new_category_dict[\"skin\"],\n",
    "                       og_category_dict[\"hair\"]:new_category_dict[\"hair\"],\n",
    "                       og_category_dict[\"glasses\"]:new_category_dict[\"glasses\"]}\n",
    "\n",
    "    if class_id in to_skin:\n",
    "        return new_category_dict[\"skin\"]\n",
    "    elif class_id in to_single_brow:\n",
    "        return new_category_dict[\"eyebrow\"]\n",
    "    elif class_id in to_single_eye:\n",
    "        return new_category_dict[\"eye\"]\n",
    "    else:\n",
    "        return identical_names[class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different segmentation methods\n",
    "\n",
    "def get_segmenation_xy(mask, class_id):\n",
    "    # Create segmentation and bbox arrays\n",
    "    mask_bool = mask == class_id\n",
    "    area = np.sum(mask_bool)\n",
    "\n",
    "    ys, xs = np.where(mask_bool)\n",
    "    segmentation = np.asarray(list(zip(xs, ys))).flatten().tolist()\n",
    "    return area, segmentation\n",
    "\n",
    "def get_segmentation_countours(mask, class_id):\n",
    "    # Create segmentation and bbox arrays\n",
    "    mask_bool = mask == class_id\n",
    "    area = np.sum(mask_bool)\n",
    "\n",
    "    retrieval_method = cv2.RETR_EXTERNAL # options: cv2.RETR_EXTERNAL, cv2.RETR_TREE\n",
    "    contour_approximation = cv2.CHAIN_APPROX_SIMPLE # options: cv2.CHAIN_APPROX_SIMPLE, cv2.CHAIN_APPROX_NONE\n",
    "    contours, hierarchy = cv2.findContours(mask_bool.astype(np.uint8), retrieval_method, contour_approximation)\n",
    "    segmentations = []\n",
    "    for contour in contours:\n",
    "        if len(contour) < 3:\n",
    "            continue\n",
    "        segmentation = []\n",
    "        for point in contour:\n",
    "            segmentation.extend(point.flatten().tolist())\n",
    "        segmentations.append(segmentation)\n",
    "    \n",
    "\n",
    "    return area, segmentations\n",
    "\n",
    "\n",
    "def get_segmentation_countours_test(mask, class_id):\n",
    "    # Create a binary mask that represents the intersection of all the masks\n",
    "    intersection_mask = np.ones_like(mask, dtype=np.uint8)\n",
    "    intersection_mask = np.logical_and(intersection_mask, mask)\n",
    "\n",
    "    # Find contours in the intersection mask\n",
    "    contours, hierarchy = cv2.findContours(intersection_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Iterate over each contour and create a segmentation mask and bounding box\n",
    "    for i in range(len(contours)):\n",
    "        # Create a binary mask for the contour\n",
    "        mask = np.zeros_like(intersection_mask)\n",
    "        cv2.drawContours(mask, contours, i, 1, -1)\n",
    "\n",
    "        # Calculate the area of the mask\n",
    "        area = int(cv2.contourArea(contours[i]))\n",
    "    \n",
    "    segmentations = []\n",
    "    for contour in contours:\n",
    "        if len(contour) < 3:\n",
    "            continue\n",
    "        segmentation = []\n",
    "        for point in contour:\n",
    "            segmentation.extend(point.flatten().tolist())\n",
    "        segmentations.append(segmentation)\n",
    "    \n",
    "\n",
    "    return area, segmentations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_segmentation_coco(image_dir):\n",
    "    non_labelled = []\n",
    "    # Loop through images in directory\n",
    "    for image_id, filename in enumerate(tqdm((os.listdir(image_dir)))):\n",
    "        if filename.endswith(tuple([\".png\", \".jpg\"])):\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            image_id+=1\n",
    "            image = cv2.imread(image_path)\n",
    "            image_dict = { \n",
    "                'id': image_id,\n",
    "                'width': image.shape[1],\n",
    "                'height': image.shape[0],\n",
    "                'file_name': filename}\n",
    "            coco_dict['images'].append(image_dict)\n",
    "            try:\n",
    "                faces, masks = get_image_pred(image, face_detector, face_parser, filename)\n",
    "                mask, face = masks[0], faces[0] # assumes 1 face per image, loop for more faces.\n",
    "                annotations_list = get_annotations_list(mask, face, image_id, coco_dict[\"annotations\"], prev_mask)\n",
    "                for dict_ in annotations_list:\n",
    "                    coco_dict['annotations'].append(dict_)\n",
    "            except RuntimeError as e:\n",
    "                non_labelled.append(get_image_info(filename))\n",
    "\n",
    "    if save_json_file:\n",
    "        save_json(coco_dict)\n",
    "\n",
    "    return sorted(non_labelled)\n",
    "\n",
    "\n",
    "def get_image_pred(img, face_detector, face_parser, filename):\n",
    "    used_df = False\n",
    "    if rotate_image:\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    faces = face_detector(img)\n",
    "    masks = face_parser.predict_img(img, faces)\n",
    "    return faces, masks\n",
    "\n",
    "def get_annotations_list(mask, face, image_id, coco_annotation_dict, prev_segmentation_mask):\n",
    "    # Create annotation dictionary for each unique mask value\n",
    "    annotation_list = [] # i am making a list of lists instead of list of dicts\n",
    "    og_category_dict = {category:id_ for id_, category in enumerate(categories_list)}\n",
    "    for class_id in np.unique(mask):\n",
    "        if class_id not in [og_category_dict[category] for category in [\"background\", \"nose\", \"left_ear\", \"right_ear\", \"upper_lip\", \"inner_mouth\", \"lower_lip\"]]:\n",
    "            annotation_id = len(coco_annotation_dict) + 1\n",
    "            annotation_dict = {\n",
    "                'id': annotation_id,\n",
    "                'image_id': image_id,\n",
    "                'category_id': convert_class_id(class_id),\n",
    "                'segmentation': [],\n",
    "                'bbox': face[:4].astype(int),\n",
    "                'iscrowd': 0}\n",
    "            area, segmentations = get_segmentation_countours(mask, class_id, prev_segmentation_mask[class_id])\n",
    "\n",
    "            # Ensure new segmentation does not overlap with previous segmentations\n",
    "            intersection_mask = np.logical_and(mask == class_id, prev_segmentation_mask[class_id] == 0)\n",
    "            segmentations = apply_mask_to_contours(segmentations, intersection_mask)\n",
    "\n",
    "            # Update previous segmentation mask\n",
    "            prev_segmentation_mask[class_id] = np.logical_or(prev_segmentation_mask[class_id], intersection_mask)\n",
    "\n",
    "            annotation_dict['segmentation'].extend(segmentations)\n",
    "            annotation_dict['area'] = area\n",
    "            annotation_list.append(annotation_dict)\n",
    "    # Add annotation to COCO dictionary\n",
    "    return annotation_list, prev_segmentation_mask\n",
    "\n",
    "\n",
    "def get_annotations_list_working(mask, face, image_id, coco_annotation_dict):\n",
    "    # Create annotation dictionary for each unique mask value\n",
    "    annotation_list = []\n",
    "    og_category_dict = {category:id_ for id_, category in enumerate(categories_list)}\n",
    "    for class_id in np.unique(mask):\n",
    "        if class_id not in [og_category_dict[category] for category in [\"background\", \"nose\", \"left_ear\", \"right_ear\", \"upper_lip\", \"inner_mouth\", \"lower_lip\"]]:\n",
    "            annotation_id = len(coco_annotation_dict) + 1\n",
    "            annotation_dict = {\n",
    "                'id': annotation_id,\n",
    "                'image_id': image_id,\n",
    "                'category_id': convert_class_id(class_id),\n",
    "                'segmentation': [],\n",
    "                'bbox': face[:4].astype(int),\n",
    "                'iscrowd': 0}\n",
    "            area, segmentations = get_segmentation_countours(mask, class_id)\n",
    "    \n",
    "            annotation_dict['segmentation'].extend(segmentations)\n",
    "            annotation_dict['area'] = area\n",
    "            annotation_list.append(annotation_dict)\n",
    "    # Add annotation to COCO dictionary\n",
    "    return annotation_list\n",
    "\n",
    "def save_json(coco_dict):\n",
    "    class NpEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            if isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "    Path(json_filepath).write_text(json.dumps(coco_dict, cls=NpEncoder, indent=3))\n",
    "    print(f\"json file written to {json_filepath}\")\n",
    "\n",
    "def get_image_info(image_filename):\n",
    "    image_nb = int(image_filename.split(\".\")[0])\n",
    "    return image_nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths and filenames\n",
    "image_dir = r'D:\\_Xchng\\Mahmoud\\segmenation\\DS01-segmentation_test\\data\\test'\n",
    "json_filepath = os.path.join(image_dir, r'instances_default.json')\n",
    "txt_filepath = os.path.join(image_dir, r'non_labeled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_annotations_list() missing 1 required positional argument: 'prev_segmentation_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m non_labelled \u001b[39m=\u001b[39m save_segmentation_coco(image_dir)\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36msave_segmentation_coco\u001b[1;34m(image_dir)\u001b[0m\n\u001b[0;32m     17\u001b[0m faces, masks \u001b[39m=\u001b[39m get_image_pred(image, face_detector, face_parser, filename)\n\u001b[0;32m     18\u001b[0m mask, face \u001b[39m=\u001b[39m masks[\u001b[39m0\u001b[39m], faces[\u001b[39m0\u001b[39m] \u001b[39m# assumes 1 face per image, loop for more faces.\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m annotations_list \u001b[39m=\u001b[39m get_annotations_list(mask, face, image_id, coco_dict[\u001b[39m\"\u001b[39;49m\u001b[39mannotations\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m dict_ \u001b[39min\u001b[39;00m annotations_list:\n\u001b[0;32m     21\u001b[0m     coco_dict[\u001b[39m'\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(dict_)\n",
      "\u001b[1;31mTypeError\u001b[0m: get_annotations_list() missing 1 required positional argument: 'prev_segmentation_mask'"
     ]
    }
   ],
   "source": [
    "non_labelled = save_segmentation_coco(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = len(os.listdir(image_dir))\n",
    "non_det = len(non_labelled)\n",
    "frac_non_det = non_det/all_\n",
    "frac_non_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_filepath, 'w') as f:\n",
    "    f.write(f\"non detected fraction: {frac_non_det}\\n\")\n",
    "    for line in sorted(non_labelled):\n",
    "        f.write(f\"{line}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_parse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
